{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9odgvxYlkGAp"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.16 --quiet\n",
    "!pip install underthesea --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install pyvi\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M980xL-CkGAq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from underthesea import word_tokenize, text_normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import gc\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUTPxNv3kGAr"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "WTb6iPIskGAr",
    "outputId": "22cbdab2-2b9b-4087-cc80-a4f4e13223ab"
   },
   "outputs": [],
   "source": [
    "# from transformers import DistilBertModel, DistilBertConfig\n",
    "# config = DistilBertConfig(\n",
    "#     num_labels=7,\n",
    "#     vocab_size=64000,\n",
    "#     max_position_embeddings=258\n",
    "# )\n",
    "# class StudentModel(nn.Module):\n",
    "#     def __init__(self, n_classes, drop_out=0.2):\n",
    "#         super(StudentModel, self).__init__()\n",
    "#         self.distillbert = DistilBertModel(config)\n",
    "#         self.dense = nn.Linear(768, 768)\n",
    "#         self.activation = nn.Tanh()\n",
    "#         self.l1 = torch.nn.Linear(768, 256)\n",
    "#         self.d1 = torch.nn.Dropout(drop_out)\n",
    "#         self.l2 = torch.nn.Linear(256, n_classes)\n",
    "#     def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, labels=None):\n",
    "#         if inputs_embeds is None:\n",
    "#             output = self.distillbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         else:\n",
    "#             output = self.distillbert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "#         output = output[0][:, 0, :]\n",
    "#         output = self.dense(output)\n",
    "#         output = self.activation(output)\n",
    "#         output = self.l1(output)\n",
    "#         output = self.d1(output)\n",
    "#         output = self.l2(output)\n",
    "#         return output\n",
    "# student_model = StudentModel(n_classes=7)\n",
    "# student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, n_classes, drop_out=0.1):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.l1 = torch.nn.Linear(768, 256)\n",
    "        self.l2 = torch.nn.Linear(256, n_classes)\n",
    "        self.d1 = torch.nn.Dropout(drop_out)\n",
    "    def forward(self,attention_mask, input_ids = None, inputs_embeds = None, labels=None):\n",
    "        if inputs_embeds is None:\n",
    "            output = self.distillbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            output = self.distillbert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        output = output[1]\n",
    "        output = self.l1(output)\n",
    "        output = self.d1(output)\n",
    "        output = self.l2(output)\n",
    "        return output\n",
    "\n",
    "student_model = StudentModel(n_classes=7)\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6nuRuTlkGAr"
   },
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, n_classes, drop_out=0.1):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "        self.l1 = torch.nn.Linear(768, 256)\n",
    "        self.l2 = torch.nn.Linear(256, n_classes)\n",
    "        self.d1 = torch.nn.Dropout(drop_out)\n",
    "    def forward(self, inputs_embeds, attention_mask, labels=None):\n",
    "        output = self.bert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        output = output[1]\n",
    "        output = self.l1(output)\n",
    "        output = self.d1(output)\n",
    "        output = self.l2(output)\n",
    "        return output\n",
    "\n",
    "teacher_model = TeacherModel(n_classes=7)\n",
    "teacher_model.to(device)\n",
    "teacher_model.load_state_dict(torch.load('/workspace/phobert_fold10.pth'), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bk7mns41kGAs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertConfig\n",
    "\n",
    "\n",
    "hidden_size = 768 \n",
    "embedding_size = hidden_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "teacher = teacher_model\n",
    "student = student_model\n",
    "\n",
    "\n",
    "optimizer_student = optim.Adam(student.parameters(), lr=1e-5)\n",
    "\n",
    "def generate_pseudo_samples(batch_size, seq_len, hidden_size, mu=0, sigma=0.35):\n",
    "    return torch.normal(mu, sigma, size=(batch_size, seq_len, hidden_size), device=device)\n",
    "\n",
    "def loss_knowledge_distillation(student_logits, teacher_logits):\n",
    "    return nn.KLDivLoss()(torch.log_softmax(student_logits, dim=-1), torch.softmax(teacher_logits, dim=-1))\n",
    "\n",
    "\n",
    "num_epochs = 5000\n",
    "batch_size = 32\n",
    "seq_len = 120  \n",
    "best_loss = 5\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    pseudo_samples = generate_pseudo_samples(batch_size, seq_len, hidden_size)\n",
    "    attention_mask = torch.ones(pseudo_samples.shape[:2], dtype=torch.long).to(device)\n",
    "\n",
    "    random_class_indices = torch.randint(0, 7, (batch_size,), device=device)\n",
    "    one_hot_targets = F.one_hot(random_class_indices, num_classes=7).float().to(device)\n",
    "\n",
    "    for i in range(25): \n",
    "        pseudo_samples.requires_grad_(True)\n",
    "        teacher_output = teacher(inputs_embeds=pseudo_samples, attention_mask=attention_mask)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(teacher_output, one_hot_targets)\n",
    "        gradient = torch.autograd.grad(loss, pseudo_samples)[0]\n",
    "        if i < 10:\n",
    "            pseudo_samples = pseudo_samples - 0.01 * gradient\n",
    "        elif 10 < i < 20:\n",
    "            pseudo_samples = pseudo_samples - 0.005 * gradient\n",
    "        else:\n",
    "            pseudo_samples = pseudo_samples - 0.001 * gradient\n",
    "\n",
    "    optimizer_student.zero_grad()\n",
    "    student_output_final = student(inputs_embeds=pseudo_samples,attention_mask=attention_mask)\n",
    "    teacher_logits = teacher(inputs_embeds=pseudo_samples, attention_mask=attention_mask)\n",
    "    loss_kd = loss_knowledge_distillation(student_output_final, teacher_logits)\n",
    "    loss_kd.backward()\n",
    "    optimizer_student.step()\n",
    "    if loss_kd < best_loss :\n",
    "        best_loss = loss_kd\n",
    "        torch.save(student_model.state_dict(), 'student_model.pth')\n",
    "    if epoch % 100 == 0 :\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Loss KD: {loss_kd.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAe5g4Uyt-PO"
   },
   "outputs": [],
   "source": [
    "class_names = ['Enjoyment', 'Disgust', 'Sadness', 'Anger', 'Surprise', 'Fear', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StudentModel(n_classes=7)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('/workspace/student_model.pth'), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6ycWJP7qQoI"
   },
   "outputs": [],
   "source": [
    "def infer(text, tokenizer, max_len=120):\n",
    "    print(f'Text: {text}')\n",
    "    text = ' '.join(simple_preprocess(text))\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = model(input_ids, attention_mask)\n",
    "    print(output.shape)\n",
    "    _, y_pred = torch.max(output, dim=1)\n",
    "\n",
    "    print(f'Sentiment: {class_names[y_pred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQo1LKipuClG"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IhoNCaPt32Q"
   },
   "outputs": [],
   "source": [
    "infer('tôi mến bạn rất nhiều', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAeuYTEWviZd"
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    df = pd.read_excel(path, sheet_name=None)['Sheet1']\n",
    "    df.columns = ['index', 'Emotion', 'Sentence']\n",
    "    # unused column\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "    return df\n",
    "test_df = get_data('/workspace/test_nor_811.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=120):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        To customize dataset, inherit from Dataset class and implement\n",
    "        __len__ & __getitem__\n",
    "        __getitem__ should return\n",
    "            data:\n",
    "                input_ids\n",
    "                attention_masks\n",
    "                text\n",
    "                targets\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "        text, label = self.get_input_data(row)\n",
    "\n",
    "        # Encode_plus will:\n",
    "        # (1) split text into token\n",
    "        # (2) Add the '[CLS]' and '[SEP]' token to the start and end\n",
    "        # (3) Truncate/Pad sentence to max length\n",
    "        # (4) Map token to their IDS\n",
    "        # (5) Create attention mask\n",
    "        # (6) Return a dictionary of outputs\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_masks': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "    def labelencoder(self,text):\n",
    "        if text=='Enjoyment':\n",
    "            return 0\n",
    "        elif text=='Disgust':\n",
    "            return 1\n",
    "        elif text=='Sadness':\n",
    "            return 2\n",
    "        elif text=='Anger':\n",
    "            return 3\n",
    "        elif text=='Surprise':\n",
    "            return 4\n",
    "        elif text=='Fear':\n",
    "            return 5\n",
    "        else:\n",
    "            return 6\n",
    "\n",
    "    def get_input_data(self, row):\n",
    "        # Preprocessing: {remove icon, special character, lower}\n",
    "        text = row['Sentence']\n",
    "        text = ' '.join(simple_preprocess(text))\n",
    "        text = ViTokenizer.tokenize(text)\n",
    "        label = self.labelencoder(row['Emotion'])\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SentimentDataset(test_df, tokenizer, max_len=50)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "losses = []\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    data_loader = test_loader\n",
    "    for data in data_loader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_masks'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        correct += torch.sum(pred == targets)\n",
    "        losses.append(loss.item())\n",
    "print(f'Test Accuracy: {correct.double()/len(test_loader.dataset)} Loss: {np.mean(losses)}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
