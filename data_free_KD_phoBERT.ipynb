{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9odgvxYlkGAp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyvi in /opt/conda/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from pyvi) (1.5.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in /opt/conda/lib/python3.11/site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (3.5.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in /opt/conda/lib/python3.11/site-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in /opt/conda/lib/python3.11/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.11/site-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.16 --quiet\n",
    "!pip install underthesea --quiet\n",
    "!pip install seaborn --quiet\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install pyvi\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M980xL-CkGAq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from underthesea import word_tokenize, text_normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import gc\n",
    "import random\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WUTPxNv3kGAr"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "WTb6iPIskGAr",
    "outputId": "22cbdab2-2b9b-4087-cc80-a4f4e13223ab"
   },
   "outputs": [],
   "source": [
    "# from transformers import DistilBertModel, DistilBertConfig\n",
    "# config = DistilBertConfig(\n",
    "#     num_labels=7,\n",
    "#     vocab_size=64000,\n",
    "#     max_position_embeddings=258\n",
    "# )\n",
    "# class StudentModel(nn.Module):\n",
    "#     def __init__(self, n_classes, drop_out=0.2):\n",
    "#         super(StudentModel, self).__init__()\n",
    "#         self.distillbert = DistilBertModel(config)\n",
    "#         self.dense = nn.Linear(768, 768)\n",
    "#         self.activation = nn.Tanh()\n",
    "#         self.l1 = torch.nn.Linear(768, 256)\n",
    "#         self.d1 = torch.nn.Dropout(drop_out)\n",
    "#         self.l2 = torch.nn.Linear(256, n_classes)\n",
    "#     def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, labels=None):\n",
    "#         if inputs_embeds is None:\n",
    "#             output = self.distillbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         else:\n",
    "#             output = self.distillbert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "#         output = output[0][:, 0, :]\n",
    "#         output = self.dense(output)\n",
    "#         output = self.activation(output)\n",
    "#         output = self.l1(output)\n",
    "#         output = self.d1(output)\n",
    "#         output = self.l2(output)\n",
    "#         return output\n",
    "# student_model = StudentModel(n_classes=7)\n",
    "# student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:1435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StudentModel(\n",
       "  (distillbert): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (l2): Linear(in_features=256, out_features=7, bias=True)\n",
       "  (d1): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, n_classes, drop_out=0.1):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.distillbert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.l1 = torch.nn.Linear(768, 256)\n",
    "        self.l2 = torch.nn.Linear(256, n_classes)\n",
    "        self.d1 = torch.nn.Dropout(drop_out)\n",
    "    def forward(self,attention_mask, input_ids = None, inputs_embeds = None, labels=None):\n",
    "        if inputs_embeds is None:\n",
    "            output = self.distillbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            output = self.distillbert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        output = output[1]\n",
    "        output = self.l1(output)\n",
    "        output = self.d1(output)\n",
    "        output = self.l2(output)\n",
    "        return output\n",
    "\n",
    "student_model = StudentModel(n_classes=7)\n",
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f6nuRuTlkGAr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2044/535071364.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  teacher_model.load_state_dict(torch.load('/workspace/teacher_model.pth'), strict = False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['bert.embeddings.position_ids'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, n_classes, drop_out=0.1):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "        self.l1 = torch.nn.Linear(768, 256)\n",
    "        self.l2 = torch.nn.Linear(256, n_classes)\n",
    "        self.d1 = torch.nn.Dropout(drop_out)\n",
    "    def forward(self, inputs_embeds, attention_mask, labels=None):\n",
    "        output = self.bert(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        output = output[1]\n",
    "        output = self.l1(output)\n",
    "        output = self.d1(output)\n",
    "        output = self.l2(output)\n",
    "        return output\n",
    "\n",
    "teacher_model = TeacherModel(n_classes=7)\n",
    "teacher_model.to(device)\n",
    "teacher_model.load_state_dict(torch.load('/workspace/teacher_model.pth'), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Bk7mns41kGAs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5000\n",
      "Loss KD: 0.09338464587926865\n",
      "Epoch 100/5000\n",
      "Loss KD: 0.019221965223550797\n",
      "Epoch 200/5000\n",
      "Loss KD: 0.015342439524829388\n",
      "Epoch 300/5000\n",
      "Loss KD: 0.013766463845968246\n",
      "Epoch 400/5000\n",
      "Loss KD: 0.016366884112358093\n",
      "Epoch 500/5000\n",
      "Loss KD: 0.019581317901611328\n",
      "Epoch 600/5000\n",
      "Loss KD: 0.01222248189151287\n",
      "Epoch 700/5000\n",
      "Loss KD: 0.016264837235212326\n",
      "Epoch 800/5000\n",
      "Loss KD: 0.01744380034506321\n",
      "Epoch 900/5000\n",
      "Loss KD: 0.017902299761772156\n",
      "Epoch 1000/5000\n",
      "Loss KD: 0.019484158605337143\n",
      "Epoch 1100/5000\n",
      "Loss KD: 0.019944138824939728\n",
      "Epoch 1200/5000\n",
      "Loss KD: 0.01846480928361416\n",
      "Epoch 1300/5000\n",
      "Loss KD: 0.011035662144422531\n",
      "Epoch 1400/5000\n",
      "Loss KD: 0.018236376345157623\n",
      "Epoch 1500/5000\n",
      "Loss KD: 0.016027409583330154\n",
      "Epoch 1600/5000\n",
      "Loss KD: 0.014961634762585163\n",
      "Epoch 1700/5000\n",
      "Loss KD: 0.015059608966112137\n",
      "Epoch 1800/5000\n",
      "Loss KD: 0.015872897580266\n",
      "Epoch 1900/5000\n",
      "Loss KD: 0.01942596770823002\n",
      "Epoch 2000/5000\n",
      "Loss KD: 0.013578692451119423\n",
      "Epoch 2100/5000\n",
      "Loss KD: 0.015193152241408825\n",
      "Epoch 2200/5000\n",
      "Loss KD: 0.015488233417272568\n",
      "Epoch 2300/5000\n",
      "Loss KD: 0.016049016267061234\n",
      "Epoch 2400/5000\n",
      "Loss KD: 0.016667503863573074\n",
      "Epoch 2500/5000\n",
      "Loss KD: 0.015261146239936352\n",
      "Epoch 2600/5000\n",
      "Loss KD: 0.012745794840157032\n",
      "Epoch 2700/5000\n",
      "Loss KD: 0.015493960119783878\n",
      "Epoch 2800/5000\n",
      "Loss KD: 0.02485494501888752\n",
      "Epoch 2900/5000\n",
      "Loss KD: 0.014140672981739044\n",
      "Epoch 3000/5000\n",
      "Loss KD: 0.017369166016578674\n",
      "Epoch 3100/5000\n",
      "Loss KD: 0.015122627839446068\n",
      "Epoch 3200/5000\n",
      "Loss KD: 0.012971768155694008\n",
      "Epoch 3300/5000\n",
      "Loss KD: 0.014115862548351288\n",
      "Epoch 3400/5000\n",
      "Loss KD: 0.010434256866574287\n",
      "Epoch 3500/5000\n",
      "Loss KD: 0.012380917556583881\n",
      "Epoch 3600/5000\n",
      "Loss KD: 0.015292482450604439\n",
      "Epoch 3700/5000\n",
      "Loss KD: 0.014192047528922558\n",
      "Epoch 3800/5000\n",
      "Loss KD: 0.015785252675414085\n",
      "Epoch 3900/5000\n",
      "Loss KD: 0.012571778148412704\n",
      "Epoch 4000/5000\n",
      "Loss KD: 0.011865591630339622\n",
      "Epoch 4100/5000\n",
      "Loss KD: 0.01190559845417738\n",
      "Epoch 4200/5000\n",
      "Loss KD: 0.012746283784508705\n",
      "Epoch 4300/5000\n",
      "Loss KD: 0.011007956229150295\n",
      "Epoch 4400/5000\n",
      "Loss KD: 0.011976588517427444\n",
      "Epoch 4500/5000\n",
      "Loss KD: 0.014370674267411232\n",
      "Epoch 4600/5000\n",
      "Loss KD: 0.01778365485370159\n",
      "Epoch 4700/5000\n",
      "Loss KD: 0.012554890476167202\n",
      "Epoch 4800/5000\n",
      "Loss KD: 0.010743916966021061\n",
      "Epoch 4900/5000\n",
      "Loss KD: 0.0122647974640131\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertConfig\n",
    "\n",
    "\n",
    "hidden_size = 768 \n",
    "embedding_size = hidden_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "teacher = teacher_model\n",
    "student = student_model\n",
    "\n",
    "\n",
    "optimizer_student = optim.Adam(student.parameters(), lr=1e-5)\n",
    "\n",
    "def generate_pseudo_samples(batch_size, seq_len, hidden_size, mu=0, sigma=1):\n",
    "    return torch.normal(mu, sigma, size=(batch_size, seq_len, hidden_size), device=device)\n",
    "\n",
    "def loss_knowledge_distillation(student_logits, teacher_logits):\n",
    "    return nn.KLDivLoss()(torch.log_softmax(student_logits, dim=-1), torch.softmax(teacher_logits, dim=-1))\n",
    "\n",
    "\n",
    "num_epochs = 5000\n",
    "batch_size = 32\n",
    "seq_len = 120  \n",
    "best_loss = 5\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    pseudo_samples = generate_pseudo_samples(batch_size, seq_len, hidden_size)\n",
    "    attention_mask = torch.ones(pseudo_samples.shape[:2], dtype=torch.long).to(device)\n",
    "    random_class_indices = torch.randint(0, 7, (batch_size,), device=device)\n",
    "    one_hot_targets = F.one_hot(random_class_indices, num_classes=7).float().to(device)\n",
    "\n",
    "    for i in range(25): \n",
    "        pseudo_samples.requires_grad_(True)\n",
    "        teacher_output = teacher(inputs_embeds=pseudo_samples, attention_mask=attention_mask)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(teacher_output, one_hot_targets)\n",
    "        gradient = torch.autograd.grad(loss, pseudo_samples)[0]\n",
    "        if i < 10:\n",
    "            pseudo_samples = pseudo_samples - 0.01 * gradient\n",
    "        elif 10 < i < 20:\n",
    "            pseudo_samples = pseudo_samples - 0.005 * gradient\n",
    "        else:\n",
    "            pseudo_samples = pseudo_samples - 0.001 * gradient\n",
    "\n",
    "    optimizer_student.zero_grad()\n",
    "    student_output_final = student(inputs_embeds=pseudo_samples,attention_mask=attention_mask)\n",
    "    teacher_logits = teacher(inputs_embeds=pseudo_samples, attention_mask=attention_mask)\n",
    "    loss_kd = loss_knowledge_distillation(student_output_final, teacher_logits)\n",
    "    loss_kd.backward()\n",
    "    optimizer_student.step()\n",
    "    if loss_kd < best_loss :\n",
    "        best_loss = loss_kd\n",
    "        torch.save(student_model.state_dict(), 'student_model_PhoBERTbase.pth')\n",
    "    if epoch % 100 == 0 :\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Loss KD: {loss_kd.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jAe5g4Uyt-PO"
   },
   "outputs": [],
   "source": [
    "class_names = ['Enjoyment', 'Disgust', 'Sadness', 'Anger', 'Surprise', 'Fear', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self, n_classes, drop_out=0.1):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        self.l1 = torch.nn.Linear(768, 256)\n",
    "        self.l2 = torch.nn.Linear(256, n_classes)\n",
    "        self.d1 = torch.nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = output[1]\n",
    "        output = self.l1(output)\n",
    "        output = self.d1(output)\n",
    "        output = self.l2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = StudentModel(n_classes=7)\n",
    "student_model.to(device)\n",
    "student_model.load_state_dict(torch.load('/workspace/student_model_PhoBERTbase.pth'), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "o6ycWJP7qQoI"
   },
   "outputs": [],
   "source": [
    "def infer(text, tokenizer, max_len=120):\n",
    "    print(f'Text: {text}')\n",
    "    text = ' '.join(simple_preprocess(text))\n",
    "    text = ViTokenizer.tokenize(text)\n",
    "\n",
    "    encoded_review = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_review['input_ids'].to(device)\n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "    output = student_model(input_ids, attention_mask)\n",
    "    print(output.shape)\n",
    "    _, y_pred = torch.max(output, dim=1)\n",
    "\n",
    "    print(f'Sentiment: {class_names[y_pred]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZQo1LKipuClG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5IhoNCaPt32Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ngày hôm nay thật đẹp \n",
      "torch.Size([1, 7])\n",
      "Sentiment: Enjoyment\n"
     ]
    }
   ],
   "source": [
    "infer('ngày hôm nay thật đẹp ', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LAeuYTEWviZd"
   },
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    df = pd.read_excel(path, sheet_name=None)['Sheet1']\n",
    "    df.columns = ['index', 'Emotion', 'Sentence']\n",
    "    # unused column\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "    return df\n",
    "test_df = get_data('/workspace/test_nor_811.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=120):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        To customize dataset, inherit from Dataset class and implement\n",
    "        __len__ & __getitem__\n",
    "        __getitem__ should return\n",
    "            data:\n",
    "                input_ids\n",
    "                attention_masks\n",
    "                text\n",
    "                targets\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "        text, label = self.get_input_data(row)\n",
    "\n",
    "        # Encode_plus will:\n",
    "        # (1) split text into token\n",
    "        # (2) Add the '[CLS]' and '[SEP]' token to the start and end\n",
    "        # (3) Truncate/Pad sentence to max length\n",
    "        # (4) Map token to their IDS\n",
    "        # (5) Create attention mask\n",
    "        # (6) Return a dictionary of outputs\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_masks': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "    def labelencoder(self,text):\n",
    "        if text=='Enjoyment':\n",
    "            return 0\n",
    "        elif text=='Disgust':\n",
    "            return 1\n",
    "        elif text=='Sadness':\n",
    "            return 2\n",
    "        elif text=='Anger':\n",
    "            return 3\n",
    "        elif text=='Surprise':\n",
    "            return 4\n",
    "        elif text=='Fear':\n",
    "            return 5\n",
    "        else:\n",
    "            return 6\n",
    "\n",
    "    def get_input_data(self, row):\n",
    "        # Preprocessing: {remove icon, special character, lower}\n",
    "        text = row['Sentence']\n",
    "        text = ' '.join(simple_preprocess(text))\n",
    "        text = ViTokenizer.tokenize(text)\n",
    "        label = self.labelencoder(row['Emotion'])\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SentimentDataset(test_df, tokenizer, max_len=50)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6161616161616161 Loss: 3.285768985748291\n"
     ]
    }
   ],
   "source": [
    "student_model.eval()\n",
    "losses = []\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    data_loader = test_loader\n",
    "    for data in data_loader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_masks'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        outputs = student_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, pred = torch.max(outputs, dim=1)\n",
    "\n",
    "        correct += torch.sum(pred == targets)\n",
    "        losses.append(loss.item())\n",
    "print(f'Test Accuracy: {correct.double()/len(test_loader.dataset)} Loss: {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
